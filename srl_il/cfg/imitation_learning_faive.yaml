defaults:
  # presets do not directly configs the pipeline or the algothm. However it provides necessary information to resolve this config
  - _self_

batch_size: 32
seed: 43
debugrun: false
wandb_cfg:
  project: robomimic
  run_name: ${now:%Y%m%d-%H%M%S}_${.project}
  tags: ["test", "act"]
  mode: "online"
output_dir: runs/${wandb_cfg.run_name}

pipeline:
  _target_: srl_il.pipeline.imitation_learning.ImitationLearningPipeline

dataset_cfg:
  data:
    _target_: srl_il.dataset.faive_dataset.faive_train_val_test
    data_directory: /home/chenyu/Data/srl/ampute_push
    test_fraction: 0.1
    val_fraction: 0.1
    window_size_train: 21 
    window_size_test: 21
    keys_traj:  [['actions', 'actions', 1, null],  # use the first frame as the observation
              ['qpos', 'qpos', 0, 2],  
              ['oakd_front_view_images', 'oakd_front_view_images', 1, 2],  
            ] 
    keys_global: []
    pad_before: false
    pad_after: true
    pad_type: 'near'
    random_seed: ${seed}
  batch_size: ${batch_size}
  pin_memory: true
  num_workers: 0


algo_cfg:
  _target_: srl_il.algo.act.ACTPolicyTrainer
  algo_cfg:
    device: cuda
    target_dims: 
      actions: 22
    z_dim: 32
    T_target: 20
    T_z: 1
    encoder_is_causal: false
    decoder_is_causal: true
    encoder_group_keys: ['qpos']
    decoder_group_keys: ['qpos', 'oakd_front_view_images']
    encoder_cfg:
      d_model: 256
      nhead: 8
      num_encoder_layers: 3
      dim_feedforward: 1024
      dropout: 0.1
      activation: 'relu'
    decoder_cfg:
      d_model: 256
      nhead: 8
      num_encoder_layers: 3
      dim_feedforward: 1024
      dropout: 0.1
      activation: 'relu'
  trainer_cfg:
    loss_params:
      kl_weight: 10
    optimizer_cfg:
      act_encoder:
        optm_cls: torch.optim.Adam
        lr: 0.0001
      act_decoder:
        optm_cls: torch.optim.Adam
        lr: 0.0001
      obs_encoder:
        optm_cls: torch.optim.Adam
        lr: 0.0001
      projs:
        optm_cls: torch.optim.Adam
        lr: 0.0001
      embeds:
        optm_cls: torch.optim.Adam
        lr: 0.0001
  obs_encoder_cfg:
    output_dim: 256
    obs_groups_cfg:
      qpos:
        datakeys: ['qpos']
        encoder_cfg:
          type: lowdim_concat
          input_dim_total: 22
        posemb_cfg:
          type: none
      oakd_front_view_images:
        datakeys: ['oakd_front_view_images']
        encoder_cfg:
          type: resnet18 # crop_resnet18
          pretrained: true
        posemb_cfg:
          type: none
    group_emb_cfg:
      type: "whole_seq_sine" # none, whole_seq_sine, each_group_learned, whole_seq_learned

  policy_cfg: 
    policy_bs: 1
    policy_obs_list: # policy names and temporal length
      - ['qpos', 2]
      - ['oakd_front_view_images', 2]


lr_scheduler_cfg:
  act_encoder:
    type: torch
    scheduler_cls: torch.optim.lr_scheduler.StepLR
    params:
      step_size: 1000
      gamma: 0.95
    step_with_metrics: false
  act_decoder:
    type: torch
    scheduler_cls: torch.optim.lr_scheduler.StepLR
    params:
      step_size: 10000
      gamma: 0.95
    step_with_metrics: false
  obs_encoder:
    type: torch
    scheduler_cls: torch.optim.lr_scheduler.StepLR
    params:
      step_size: 10000
      gamma: 0.95
    step_with_metrics: false

training_cfg:
  num_epochs: 500
  num_steps_per_epoch: 200
  steps_saving: 10
  rollout:
    enabled: false


normalizer_cfg:
  actions: # the target to reconstruct
    type: dataset_stats
    min_max: true
    dataname: actions
  qpos:
    type: dataset_stats
    dataname: qpos
  oakd_front_view_images: # image net norm mean = [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    type: hardcode
    mean: [[[0.485]], [[0.456]], [[0.406]]]
    std: [[[0.229]], [[0.224]], [[0.225]]]

data_augmentation_cfg: # data augmetation is only used for the data loaded from dataset, no simulation data augmentation
  data_augments:
    - outname: qpos
      type: gaussian_noise
      mean: 0.0
      std: 0.01


sim_env_cfg: {}
    
 
# set the directory where the output files get saved
hydra:
  output_subdir: ${output_dir}/hydra
  run:
    dir: .
